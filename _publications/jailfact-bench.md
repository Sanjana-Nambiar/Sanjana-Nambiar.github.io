---
title: "JailFact-Bench: A Comprehensive Analysis of Jailbreak Attacks vs. Hallucinations in LLMs"
collection: publications
category: conferences
permalink: /publication/jailfact-bench
excerpt: "Benchmarks jailbreak attacks on LLMs and investigates their potential to induce hallucinations, proposing categorization strategies and countermeasures to strengthen AI alignment."
date: 2025-06-24
venue: 'SiMLA 2025 Workshop @ ACNS (to appear in Springer LNCS post-proceedings)'
header:
  teaser: publications/jailfact-bench.png 
paperurl: '/files/SiMLA-2-main.pdf'
slidesurl: 'https://docs.google.com/presentation/d/1YXF5duFIcuPsV7E2arLNwX-EGhgvRXhgTqOktDS_XZE/edit?usp=sharing'
citation: 'Sanjana Nambiar, Christina PÃ¶pper. (2025). "JailFact-Bench: A Comprehensive Analysis of Jailbreak Attacks vs. Hallucinations in LLMs." <i>SiMLA 2025 Workshop, co-located with ACNS 2025</i>. To appear in Springer LNCS.'
---
This paper investigates the relationship between jailbreak prompts and hallucination behavior in large language models (LLMs). It introduces JailFact-Bench, a benchmark designed to evaluate factual accuracy under adversarial prompting scenarios. Through semantic similarity and factual precision scoring, the study reveals that many jailbreaks induce hallucinations, challenging conventional boundaries between safety and truthfulness in LLMs. The research was presented at the SiMLA 2025 Workshop, co-located with ACNS 2025, and will be published in the Springer LNCS post-proceedings.

